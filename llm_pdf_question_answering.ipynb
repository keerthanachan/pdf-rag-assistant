Build a Mini LLM-Powered Question-Answering System Using RAG

# Commented out IPython magic to ensure Python compatibility.
# %pip install PyMuPDF

# Commented out IPython magic to ensure Python compatibility.
# %pip install faiss-cpu

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-community

import fitz # For reading and extracting text from PDF files (PyMuPDF)
import numpy as np # For numerical operations and array handling
import faiss # For fast similarity search using vector embeddings
from sentence_transformers import SentenceTransformer # For generating sentence embeddings from text
from langchain.text_splitter import RecursiveCharacterTextSplitter # For splitting large texts into manageable chunks
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline  # For loading and using Hugging Face transformer models
from langchain_community.llms import HuggingFacePipeline # For integrating Hugging Face models with LangChain

embedding_model = SentenceTransformer('all-MiniLM-L6-v2') # Load a lightweight sentence embedding model for semantic search
llm_model_id = "google/flan-t5-small"  # Define the model ID for the FLAN-T5 small language model
tokenizer = AutoTokenizer.from_pretrained(llm_model_id) # Load the tokenizer corresponding to the FLAN-T5 model
llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_id) # Load the FLAN-T5 model for sequence-to-sequence tasks
llm_pipe = pipeline("text2text-generation", model=llm_model, tokenizer=tokenizer, max_new_tokens=200) # Create a text generation pipeline with token limit
llm = HuggingFacePipeline(pipeline=llm_pipe)  # Wrap the pipeline for use in LangChain-compatible LLM interface

# Function to extract and return plain text from all pages of a PDF file using PyMuPDF
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def chunk_text(text, chunk_size=500, chunk_overlap=50):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)

# Function to create a FAISS vector store from text chunks using sentence embeddings for similarity search
def create_vector_store(chunks):
    embeddings = embedding_model.encode(chunks, show_progress_bar=False)
    embeddings_np = np.array(embeddings)
    index = faiss.IndexFlatL2(embeddings_np.shape[1])
    index.add(embeddings_np)
    return index, chunks, embedding_model

# Function to retrieve the top-k most similar text chunks to a query using FAISS and embedding model
def retrieve_top_k(query, model, index, chunks, k=3):
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    return [chunks[i] for i in I[0]]

# Function to process a PDF and query, extract text, retrieve relevant chunks using vector search, and generate an answer using an LLM
def generate_answer(pdf_file, query):
    print("Starting generate_answer function...")
    try:
        print("Extracting text from PDF...")
        raw_text = extract_text_from_pdf(pdf_file)
        print("Text extraction complete.")

        print(f"Raw text length: {len(raw_text)}")
        if not raw_text:
            print("Extracted text is empty.")
            return "Could not extract text from the PDF."

        print("Chunking text...")
        chunks = chunk_text(raw_text)
        print(f"Number of chunks: {len(chunks)}")
        if not chunks:
             print("Text chunking resulted in no chunks.")
             return "Could not chunk the text from the PDF."

        print("Creating vector store...")
        # Ensure global models are accessible or passed
        global embedding_model
        if 'embedding_model' not in globals():
             print("Error: embedding_model not found in globals.")
             return "Embedding model not loaded."

        index, chunk_list, embed_model = create_vector_store(chunks)
        print("Vector store created.")

        print(f"Query: {query}")
        if not query:
            print("Query is empty.")
            return "Please provide a question."

        print("Retrieving top k relevant chunks...")
        relevant_chunks = retrieve_top_k(query, embed_model, index, chunk_list)
        print(f"Number of relevant chunks found: {len(relevant_chunks)}")
        if not relevant_chunks:
            print("No relevant chunks found for the query.")
            # Optionally return a message or proceed with a different strategy
            return "Could not find relevant information in the document for your question."


        context = "\n".join(relevant_chunks)
        print(f"Context for LLM (first 200 chars): {context[:200]}...")

        prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
        print(f"Prompt for LLM (first 200 chars): {prompt[:200]}...")

        print("Generating answer with LLM...")
        # Ensure global llm is accessible or passed
        global llm
        if 'llm' not in globals():
             print("Error: llm not found in globals.")
             return "LLM model not loaded."

        answer = llm(prompt)
        print("Answer generated.")
        return answer
    except Exception as e:
        print(f"An error occurred during processing: {e}")
        return f"An error occurred: {e}"

# Create and launch a Gradio web app to upload a PDF and ask questions answered using a RAG-based mini LLM
import gradio as gr

interface = gr.Interface(
    fn=generate_answer,
    inputs=[
        gr.File(label="/content/9241544228_eng.pdf"),
        gr.Textbox(label="Give me the correct coded classification for the following diagnosis?")
    ],
    outputs="text",
    title="ðŸ“„ Mini LLM QA using RAG",
    description="Upload a PDF document and ask questions about its conten"
)

interface.launch()

def extract_text_from_pdf(pdf_file_path):
    # Open the PDF file using the provided file path
    doc = fitz.open(pdf_file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close() # Close the document after processing
    return text

